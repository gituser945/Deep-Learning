{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Natural Language Processing\n",
    "#### Story Generation Implementation using tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing important libraries\n",
    "\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading file for training\n",
    "\n",
    "with open('book.txt','r') as f:\n",
    "    text = f.read()\n",
    "vocab = set(text) # using set we are identifying unique characters from the file\n",
    "vocab_to_int = {c : i for i,c in enumerate(vocab)} # by using enumeration we are retrieving correspondig numbers with alph.\n",
    "int_to_vocab = dict(enumerate(vocab))\n",
    "char = np.array([vocab_to_int[c] for c in text],dtype = np.int32) # converting all letters to corresponding numerical value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When most people hear “Machine Learning,” they picture a robot: a dependable butler or a deadly Terminator, depending on who you ask. But Machine Learning is not just a futuristic fantasy; it’s already here. In fact, it has been around for decades in some specialized applications, such as Optical Character Recognition (OCR). But the first ML application that really became mainstream, improving the lives of hundreds of millions of people, took over the world back in the 1990s: the spam filter. It’s not exactly a self-aware Skynet, but it does technically qualify as Machine Learning (it has actually learned so well that you seldom need to flag an email as spam anymore). It was followed by hundreds of ML applications that now quietly power hundreds of products and features that you use regularly, from better recommendations to voice search.\\nWhere does Machine Learning start and where does it end? What exactly does it mean for a machine to learn something? If I download a copy of Wikipedia, has my computer really learned something? Is it suddenly smarter? In this chapter we will start by clarifying what Machine Learning is and why you may want to use it.\\nThen, before we set out to explore the Machine Learning continent, we will take a look at the map and learn about the main regions and the most notable landmarks: supervised versus unsupervised learning, online versus batch learning, instance-based versus model-based learning. Then we will look at the workflow of a typical ML project, discuss the main challenges you may face, and cover how to evaluate and fine-tune a Machine Learning system.\\nThis chapter introduces a lot of fundamental concepts (and jargon) that every data scientist should know by heart. It will be a high-level overview (it’s the only chapter without much code), all rather simple, but you should What Is Machine Learning?\\nMachine Learning is the science (and art) of programming computers so they can learn from data.\\nHere is a slightly more general definition:\\n[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed.\\n—Arthur Samuel, 1959\\nAnd a more engineering-oriented one:\\nA computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\\n—Tom Mitchell, 1997\\nYour spam filter is a Machine Learning program that, given examples of spam emails (e.g., flagged by users) and examples of regular (nonspam, also called “ham”) emails, can learn to flag spam. The examples that the system uses to learn are called the training set. Each training example is called a training instance (or sample). In this case, the task T is to flag spam for new emails, the experience E is the training data, and the performance measure P needs to be defined; for example, you can use the ratio of correctly classified emails. This particular performance measure is called accuracy, and it is often used in classification tasks.\\nWhy Use Machine Learning?\\nConsider how you would write a spam filter using traditional programming techniques (Figure 1-1):\\n1. First you would consider what spam typically looks like. You might notice that some words or phrases (such as “4U,” “credit card,” “free,” and “amazing”) tend to come up a lot in the subject line. Perhaps you would also notice a few other patterns in the sender’s name, the email’s body, and other parts of the email.\\n2. You would write a detection algorithm for each of the patterns that you noticed, and your program would flag emails as spam if a number of these patterns were detected.\\n3. You would test your program and repeat steps 1 and 2 until it was good enough to launch.Since the problem is difficult, your program will likely become a long list of complex rules—pretty hard to maintain.\\nIn contrast, a spam filter based on Machine Learning techniques automatically learns which words and phrases are good predictors of spam by detecting unusually frequent patterns of words in the spam examples compared to the ham examples (Figure 1-2). The program is much shorter, easier to maintain, and most likely more accurate.\\nWhat if spammers notice that all their emails containing “4U” are blocked? They might start writing “For U” instead. A spam filter using traditional programming techniques would need to be updated to flag “For U” emails. If spammers keep working around your spam filter, you will need to keep writing new rules forever.\\nIn contrast, a spam filter based on Machine Learning techniques automatically notices that “For U” has become unusually frequent in spam flagged by users, and it starts flagging them without your intervention Another area where Machine Learning shines is for problems that either are too complex for traditional approaches or have no known algorithm. For example, consider speech recognition. Say you want to start simple and write a program capable of distinguishing the words “one” and “two.” You might notice that the word “two” starts with a high-pitch sound (“T”), so you could hardcode an algorithm that measures high-pitch sound intensity and use that to distinguish ones and twos—but obviously this technique will not scale to thousands of words spoken by millions of very different people in noisy environments and in dozens of languages. The best solution (at least today) is to write an algorithm that learns by itself, given many example recordings for each word.\\nFinally, Machine Learning can help humans learn (Figure 1-4). ML algorithms can be inspected to see what they have learned (although for some algorithms this can be tricky). For instance, once a spam filter has been trained on enough spam, it can easily be inspected to reveal the list of words and combinations of words that it believes are the best predictors of spam. Sometimes this will reveal unsuspected correlations or new trends, and thereby lead to a better understanding of the problem. Applying ML techniques to dig into large amounts of data can help discover patterns that were not immediately apparent. This is called data mining.Recommending a product that a client may be interested in, based on past purchases\\nThis is a recommender system. One approach is to feed past purchases (and other information about the client) to an artificial neural network (see Chapter 10), and get it to output the most likely next purchase. This neural net would typically be trained on past sequences of purchases across all clients.\\nBuilding an intelligent bot for a game\\nThis is often tackled using Reinforcement Learning (RL; see\\nChapter 18), which is a branch of Machine Learning that trains agents (such as bots) to pick the actions that will maximize their rewards over time (e.g., a bot may get a reward every time the player loses some life points), within a given environment (such as the game). The famous AlphaGo program that beat the world champion at the game of Go was built using RL.\\nThis list could go on and on, but hopefully it gives you a sense of the incredible breadth and complexity of the tasks that Machine Learning can tackle, and the types of techniques that you would use for each task.\\nTypes of Machine Learning Systems\\nThere are so many different types of Machine Learning systems that it is useful to classify them in broad categories, based on the following criteria:\\nWhether or not they are trained with human supervision (supervised, unsupervised, semisupervised, and Reinforcement Learning)\\nWhether or not they can learn incrementally on the fly (online versus batch learning)\\nWhether they work by simply comparing new data points to known data points, or instead by detecting patterns in the training data and\\nbuilding a predictive model, much like scientists do (instance- based versus model-based learning)\\nThese criteria are not exclusive; you can combine them in any way you like. For example, a state-of-the-art spam filter may learn on the fly using a deep neural network model trained using examples of spam and ham; this makes it an online, model-based, supervised learning system.\\nLet’s look at each of these criteria a bit more closely.\\nSupervised/Unsupervised Learning\\nMachine Learning systems can be classified according to the amount and type of supervision they get during training. There are four major categories: supervised learning, unsupervised learning, semisupervised learning, and Reinforcement Learning.\\nSupervised learning\\nIn supervised learning, the training set you feed to the algorithm includes the desired solutions, called labels (Figure 1-5).A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails.\\nAnother typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors.This sort of task is called regression (Figure 1-6).1 To train the system, you need to give it many examples of cars, including both their predictors and their labels (i.e., their prices).Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam).For example, say you have a lot of data about your blog’s visitors. You may want to run a clustering algorithm to try to detect groups of similar visitors (Figure 1-8). At no point do you tell the algorithm which group a visitor belongs to: it finds those connections without your help. For example, it might notice that 40% of your visitors are males who love comic books and generally read your blog in the evening, while 20% are young sci-fi lovers who visit during the weekends. If you use a hierarchical clustering algorithm, it may also subdivide each group into smaller groups. This may help you target your posts for each group.Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D representation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization) so that you can understand how the data is organized and perhaps identify unsuspected patterns.A related task is dimensionality reduction, in which the goal is to simplify the data without losing too much information. One way to do this is to merge several correlated features into one. For example, a car’s mileage may be strongly correlated with its age, so the dimensionality reduction algorithm will merge them into one feature that represents the car’s wear and tear. This is called feature extraction.\\nTIP\\nIt is often a good idea to try to reduce the dimension of your training data using a dimensionality reduction algorithm before you feed it to another Machine Learning algorithm (such as a supervised learning algorithm). It will run much faster, the data will take up less disk and memory space, and in some cases it may also perform better.\\nYet another important unsupervised task is anomaly detection—for example, detecting unusual credit card transactions to prevent fraud, catching manufacturing defects, or automatically removing outliers from a\\ndataset before feeding it to another learning algorithm. The system is shown mostly normal instances during training, so it learns to recognize them; then, when it sees a new instance, it can tell whether it looks like a normal one or whether it is likely an anomaly (see Figure 1-10). A very similar task is novelty detection: it aims to detect new instances that look different from all instances in the training set. This requires having a very “clean” training set, devoid of any instance that you would like the algorithm to detect. For example, if you have thousands of pictures of dogs, and 1% of these pictures represent Chihuahuas, then a novelty detection algorithm should not treat new pictures of Chihuahuas as novelties. On the other hand, anomaly detection algorithms may consider these dogs as so rare and so different from other dogs that they would likely classify them as anomalies (no offense to Chihuahuas).\\nFigure 1-10. Anomaly detection\\nFinally, another common unsupervised task is association rule learning, in which the goal is to dig into large amounts of data and discover interesting relations between attributes. For example, suppose you own a supermarket. Running an association rule on your sales logs may reveal that people who purchase barbecue sauce and potato chips also tend to buy steak. Thus, you may want to place these items close to one another.\\nSemisupervised learning Since labeling data is usually time-consuming and costly, you will often have plenty of unlabeled instances, and few labeled instances. Some algorithms can deal with data that’s partially labeled. This is called semisupervised learning (Figure 1-11).\\nFigure 1-11. Semisupervised learning with two classes (triangles and squares): the unlabeled examples (circles) help classify a new instance (the cross) into the triangle class rather than the square class, even though it is closer to the labeled squares\\nSome photo-hosting services, such as Google Photos, are good examples of this. Once you upload all your family photos to the service, it automatically recognizes that the same person A shows up in photos 1, 5, and 11, while another person B shows up in photos 2, 5, and 7. This is the unsupervised part of the algorithm (clustering). Now all the system needs is for you to tell it who these people are. Just add one label per person4 and it is able to name everyone in every photo, which is useful for searching photos.\\nMost semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsupervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques.\\nReinforcement Learning\\n \\n Reinforcement Learning isag a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as shown in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.\\nFigure 1-12. Reinforcement Learning\\nFor example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind’s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the games against the champion; AlphaGo was just applying the policy it had learned.\\nBatch and Online Learning\\nAnother criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data.\\nBatch learning\\nIn batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called offline learning.\\nIf you want a batch learning system to know about new data (such as a new type of spam), you need to train a new version of the system from scratch on the full dataset (not just the new data, but also the old data), then stop the old system and replace it with the new one.\\nFortunately, the whole process of training, evaluating, and launching a Machine Learning system can be automated fairly easily (as shown in Figure 1-3), so even a batch learning system can adapt to change. Simply update the data and train a new version of the system from scratch as often as needed.\\nThis solution is simple and often works fine, but training using the full set of data can take many hours, so you would typically train a new system only every 24 hours or even just weekly. If your system needs to adapt to rapidly changing data (e.g., to predict stock prices), then you need a more reactive solution.\\nAlso, training on the full set of data requires a lot of computing resources (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and you automate your system to train from scratch every day, it\\nwill end up costing you a lot of money. If the amount of data is huge, it may even be impossible to use a batch learning algorithm.\\nFinally, if your system needs to be able to learn autonomously and it has limited resources (e.g., a smartphone application or a rover on Mars), then carrying around large amounts of training data and taking up a lot of resources to train for hours every day is a showstopper.\\nFortunately, a better option in all these cases is to use algorithms that are capable of learning incrementally.\\nOnline learning\\nIn online learning, you train the system incrementally by feeding it data instances sequentially, either individually or in small groups called mini- batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives (see Figure 1-13).\\nFigure 1-13. In online learning, a model is trained and launched into production, and then it keeps learning as new data comes in\\nOnline learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously. It is also a good option if you have limited computing resources: once an online learning system has learned about new data instances, it does not need them anymore, so you can discard them (unless you want to be able to roll back to a previous state and “replay” the data). This can save a huge amount of space.\\nOnline learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine’s main memory (this is called out-of- core learning). The algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14).\\nWARNING\\nOut-of-core learning is usually done offline (i.e., not on the live system), so online learning can be a confusing name. Think of it as incremental learning.\\nOne important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data (you don’t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it will learn more slowly, but it will also be less sensitive to noise in the new data or to sequences of nonrepresentative data points (outliers).\\n Figure 1-14. Using online learning to handle huge datasets\\nA big challenge with online learning is that if bad data is fed to the system, the system’s performance will gradually decline. If it’s a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from someone spamming a search engine to try to rank high in search results. To reduce this risk, you need to monitor your system closely and promptly switch learning off (and possibly revert to a previously working state) if you detect a drop in performance. You may also want to monitor the input data and react to abnormal data (e.g., using an anomaly detection algorithm).\\nInstance-Based Versus Model-Based Learning\\nOne more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means that given a number of training examples, the system needs to be able to make good predictions for (generalize to) examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal is to perform well on new instances.There are two main approaches to generalization: instance-based learning and model-based learning.\\nInstance-based learning\\nPossibly the most trivial form of learning is simply to learn by heart. If you were to create a spam filter this way, it would just flag all emails that are identical to emails that have already been flagged by users—not the worst solution, but certainly not the best.\\nInstead of just flagging emails that are identical to known spam emails, your spam filter could be programmed to also flag emails that are very similar to known spam emails. This requires a measure of similarity between two emails. A (very basic) similarity measure between two emails could be to count the number of words they have in common. The system would flag an email as spam if it has many words in common with a known spam email.\\nThis is called instance-based learning: the system learns the examples by heart, then generalizes to new cases by using a similarity measure to compare them to the learned examples (or a subset of them). For example, in Figure 1-15 the new instance would be classified as a triangle because the majority of the most similar instances belong to that class.\\nFigure 1-15. Instance-based learning\\n \\nModel-based learning\\nAnother way to generalize from a set of examples is to build a model of these examples and then use that model to make predictions. This is called model-based learning (Figure 1-16).\\nFigure 1-16. Model-based learning\\nFor example, suppose you want to know if money makes people happy, so you download the Better Life Index data from the OECD’s website and stats about gross domestic product (GDP) per capita from the IMF’s website. Then you join the tables and sort by GDP per capita. Table 1-1 shows an excerpt of what you get.There does seem to be a trend here! Although the data is noisy (i.e., partly random), it looks like life satisfaction goes up more or less linearly as the country’s GDP per capita increases. So you decide to model life satisfaction as a linear function of GDP per capita. This step is called model selection: you selected a linear model of life satisfaction with just one attribute, GDP per capita (Equation 1-1).\\nEquation 1-1. A simple linear model\\nThis model has two model parameters, θ0 and θ1.5 By tweaking these parameters, you can make your model represent any linear function, as shown in Figure 1-18.\\n atipac_rep_PDG × 1θ + 0θ = noitcafsitas_efil\\n Figure 1-18. A few possible linear models\\nBefore you can use your model, you need to define the parameter values θ0 and θ1. How can you know which values will make your model perform best? To answer this question, you need to specify a performance measure. You can either define a utility function (or fitness function) that measures how good your model is, or you can define a cost function that measures how bad it is. For Linear Regression problems, people typically use a cost function that measures the distance between the linear model’s predictions and the training examples; the objective is to minimize this distance.\\nThis is where the Linear Regression algorithm comes in: you feed it your training examples, and it finds the parameters that make the linear model fit best to your data. This is called training the model. In our case, the algorithm finds that the optimal parameter values are θ0 = 4.85 and θ1 = 4.91 × 10–5.If all went well, your model will make good predictions. If not, you may need to use more attributes (employment rate, health, air pollution, etc.), get more or better-quality training data, or perhaps select a more powerful model (e.g., a Polynomial Regression model).\\nIn summary:\\nYou studied the data.\\nYou selected a model.\\nYou trained it on the training data (i.e., the learning algorithm searched for the model parameter values that minimize a cost function).\\nFinally, you applied the model to make predictions on new cases (this is called inference), hoping that this model will generalize\\nNOTE\\nIf you had used an instance-based learning algorithm instead, you would have found that Slovenia has the closest GDP per capita to that of Cyprus ($20,732), and since the OECD data tells us that Slovenians’ life satisfaction is 5.7, you would have predicted a life satisfaction of 5.7 for Cyprus. If you zoom out a bit and look at the two next-closest countries, you will find Portugal and Spain with life satisfactions of 5.1 and 6.5, respectively. Averaging these three values, you get 5.77, which is pretty close to your model-based prediction. This simple algorithm is called k-Nearest Neighbors regression (in this example, k = 3).\\nReplacing the Linear Regression model with k-Nearest Neighbors regression in the previous code is as simple as replacing these two lines:\\nwith these two:\\n                                )3=srobhgien_n\\n(rossergeRsrobhgieNK.srobhgien.nraelks = ledom\\n                      srobhgien.nraelks tropmi\\n)(noissergeRraeniL.ledom_raenil.nraelks = ledom\\n                    ledom_raenil.nraelks tropmi\\nwell.\\nThis is what a typical Machine Learning project looks like. In Chapter 2 you will experience this firsthand by going through a project end to end.\\nWe have covered a lot of ground so far: you now know what Machine Learning is really about, why it is useful, what some of the most common categories of ML systems are, and what a typical project workflow looks like. Now let’s look at what can go wrong in learning and prevent you from making accurate predictions.\\nMain Challenges of Machine Learning\\nIn short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are “bad algorithm” and “bad data.” Let’s start with examples of bad data.\\nInsufficient Quantity of Training Data\\nFor a toddler to learn what an apple is, all it takes is for you to point to an apple and say “apple” (possibly repeating this procedure a few times). Now the child is able to recognize apples in all sorts of colors and shapes. Genius.\\nMachine Learning is not quite there yet; it takes a lot of data for most Machine Learning algorithms to work properly. Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recognition you may need millions of examples (unless you can reuse parts of an existing model). THE UNREASONABLE EFFECTIVENESS OF DATA\\nIn a famous paper published in 2001, Microsoft researchers Michele Banko and Eric Brill showed that very different Machine Learning algorithms, including fairly simple ones, performed almost identically well on a complex problem of natural language disambiguation8 once they were given enough data (as you can see in Figure 1-20).\\nFigure 1-20. The importance of data versus algorithms9\\nAs the authors put it, “these results suggest that we may want to reconsider the trade-off between spending time and money on algorithm development versus spending it on corpus development.”\\nNonrepresentative Training Data\\nIn order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning.\\nFor example, the set of countries we used earlier for training the linear model was not perfectly representative; a few countries were missing. Figure 1-21 shows what the data looks like when you add the missing countries.\\nFigure 1-21. A more representative training sample\\nIf you train a linear model on this data, you get the solid line, while the old model is represented by the dotted line. As you can see, not only does adding a few missing countries significantly alter the model, but it makes it clear that such a simple linear model is probably never going to work well. It seems that very rich countries are not happier than moderately rich countries (in fact, they seem unhappier), and conversely some poor countries seem happier than many rich countries.By using a nonrepresentative training set, we trained a model that is unlikely to make accurate predictions, especially for very poor and very rich countries.\\nIt is crucial to use a training set that is representative of the cases you want to generalize to. This is often harder than it sounds: if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.\\nPoor-Quality Data\\nEXAMPLES OF SAMPLING BIAS\\nPerhaps the most famous example of sampling bias happened during the US presidential election in 1936, which pitted Landon against Roosevelt: the Literary Digest conducted a very large poll, sending mail to about 10 million people. It got 2.4 million answers, and predicted with high confidence that Landon would get 57% of the votes. Instead, Roosevelt won with 62% of the votes. The flaw was in the Literary Digest’s sampling method:\\nFirst, to obtain the addresses to send the polls to, the Literary Digest used telephone directories, lists of magazine subscribers, club membership lists, and the like. All of these lists tended to favor wealthier people, who were more likely to vote Republican (hence Landon).\\nSecond, less than 25% of the people who were polled answered. Again this introduced a sampling bias, by potentially ruling out people who didn’t care much about politics, people who didn’t like the Literary Digest, and other key groups. This is a special type of sampling bias called nonresponse bias.\\nHere is another example: say you want to build a system to recognize funk music videos. One way to build your training set is to search for “funk music” on YouTube and use the resulting videos. But this assumes that YouTube’s search engine returns a set of videos that are representative of all the funk music videos on YouTube. In reality, the search results are likely to be biased toward popular artists (and if you live in Brazil you will get a lot of “funk carioca” videos, which sound nothing like James Brown). On the other hand, how else can you get a large training set?Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor-quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. The following are a couple of examples of when you’d want to clean up training data:\\nIf some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.\\nIf some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute altogether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it.\\nIrrelevant Features\\nAs the saying goes: garbage in, garbage out. Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves the following steps:\\nFeature selection (selecting the most useful features to train on among existing features)\\nFeature extraction (combining existing features to produce a more useful one—as we saw earlier, dimensionality reduction algorithms can help)\\nCreating new features by gathering new data\\nNow that we have looked at many examples of bad data, let’s look at a couple of examples of bad algorithms.\\nOverfitting the Training Data\\nSay you are visiting a foreign country and the taxi driver rips you off. You might be tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overfitting: it means that the model performs well on the training data, but it does not generalize well.\\nFigure 1-22 shows an example of a high-degree polynomial life satisfaction model that strongly overfits the training data. Even though it performs much better on the training data than the simple linear model, would you really trust its predictions?\\nFigure 1-22. Overfitting the training data\\nComplex models such as deep neural networks can detect subtle patterns in the data, but if the training set is noisy, or if it is too small (which introduces sampling noise), then the model is likely to detect patterns in the noise itself. Obviously these patterns will not generalize to new instances. For example, say you feed your life satisfaction model many more attributes, including uninformative ones such as the country’s name. In that case, a complex model may detect patterns like the fact that all countries in the training data with a w in their name have a life satisfaction greater than 7: New Zealand (7.3), Norway (7.4), Sweden (7.2), and Switzerland (7.5). How confident are you that the w-satisfaction rule generalizes to Rwanda or Zimbabwe? Obviously this pattern occurred in the training data by pure chance, but the model has no way to tell whether a pattern is real or simply the result of noise in the data.\\nWARNING\\nOverfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:\\nSimplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data, or by constraining the model.\\nGather more training data.\\nReduce the noise in the training data (e.g., fix data errors and remove outliers).\\nConstraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, θ0 and θ1. This gives the learning algorithm two degrees of freedom to adapt the model to the training data: it can tweak both the height (θ0) and the slope (θ1) of the line. If we forced θ1 = 0, the algorithm would have only one degree of freedom and would have a much harder time fitting the data properly: all it could do is move the line up or down to get as close as possible to the training instances, so it would end up around the mean. A very simple model indeed! If we allow the algorithm to modify θ1 but we force it to keep it small, then the learning algorithm will effectively have somewhere in between one and two degrees of freedom. It will produce a model that’s simpler than one with two degrees of freedom, but more complex than one with just one. You want to find the right balance between fitting the training data perfectly and keeping the model simple enough to ensure that it will generalize well.\\nFigure 1-23 shows three models. The dotted line represents the original model that was trained on the countries represented as circles (without the countries represented as squares), the dashed line is our second model trained with all countries (circles and squares), and the solid line is a model\\ntrained with the same data as the first model but with a regularization constraint. You can see that regularization forced the model to have a smaller slope: this model does not fit the training data (circles) as well as the first model, but it actually generalizes better to new examples that it did not see during training (squares).\\nFigure 1-23. Regularization reduces the risk of overfitting\\nThe amount of regularization to apply during learning can be controlled by a hyperparameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyperparameter to a very large value, you will get an almost flat model (a slope close to zero); the learning algorithm will almost certainly not overfit the training data, but it will be less likely to find a good solution. Tuning hyperparameters is an important part of building a Machine Learning system (you will see a detailed example in the next chapter).\\nUnderfitting the Training Data\\nAs you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a linear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.\\nHere are the main options for fixing this problem Select a more powerful model, with more parameters.\\nFeed better features to the learning algorithm (feature engineering).\\nReduce the constraints on the model (e.g., reduce the regularization hyperparameter).\\nStepping Back\\nBy now you know a lot about Machine Learning. However, we went through so many concepts that you may be feeling a little lost, so let’s step back and look at the big picture:\\nMachine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.\\nThere are many different types of ML systems: supervised or not, batch or online, instance-based or model-based.\\nIn an ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model- based, it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and generalizes to new instances by using a similarity measure to compare them to the learned instances.\\nThe system will not perform well if your training set is too small, or if the data is not representative, is noisy, or is polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).\\nThere’s just one last important topic to cover: once you have trained a model, you don’t want to just “hope” it generalizes to new cases. You want to evaluate it and fine-tune it if necessary. Let’s see how to do that.\\nTesting and Validating\\nThe only way to know how well a model will generalize to new cases is to actually try it out on new cases. One way to do that is to put your model in production and monitor how well it performs. This works well, but if your model is horribly bad, your users will complain—not the best idea.\\nA better option is to split your data into two sets: the training set and the test set. As these names imply, you train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-of-sample error), and by evaluating your model on the test set, you get an estimate of this error. This value tells you how well your model will perform on instances it has never seen before.\\nIf the training error is low (i.e., your model makes few mistakes on the training set) but the generalization error is high, it means that your model is overfitting the training data.\\nTIP\\nIt is common to use 80% of the data for training and hold out 20% for testing. However, this depends on the size of the dataset: if it contains 10 million instances, then holding out 1% means your test set will contain 100,000 instances, probably more than enough to get a good estimate of the generalization error.\\nHyperparameter Tuning and Model Selection\\nEvaluating a model is simple enough: just use a test set. But suppose you are hesitating between two types of models (say, a linear model and a polynomial model): how can you decide between them? One option is to train both and compare how well they generalize using the test set.\\nNow suppose that the linear model generalizes better, but you want to apply some regularization to avoid overfitting. The question is, how do you choose the value of the regularization hyperparameter? One option is to train 100 different models using 100 different values for this hyperparameter. Suppose you find the best hyperparameter value that produces a model with the lowest generalization error—say, just 5% error. You launch this model into production, but unfortunately it does not perform as well as expected and produces 15% errors. What just happened?\\nThe problem is that you measured the generalization error multiple times on the test set, and you adapted the model and hyperparameters to produce the best model for that particular set. This means that the model is unlikely to perform as well on new data.\\nA common solution to this problem is called holdout validation: you simply hold out part of the training set to evaluate several candidate models and select the best one. The new held-out set is called the validation set (or sometimes the development set, or dev set). More specifically, you train multiple models with various hyperparameters on the reduced training set (i.e., the full training set minus the validation set), and you select the model that performs best on the validation set. After this holdout validation process, you train the best model on the full training set (including the validation set), and this gives you the final model. Lastly, you evaluate this final model on the test set to get an estimate of the generalization error.\\nThis solution usually works quite well. However, if the validation set is too small, then model evaluations will be imprecise: you may end up selecting a suboptimal model by mistake. Conversely, if the validation set is too large, then the remaining training set will be much smaller than the full training set. Why is this bad? Well, since the final model will be trained on the full training set, it is not ideal to compare candidate models trained on a much smaller training set. It would be like selecting the fastest sprinter to participate in a marathon. One way to solve this problem is to perform repeated cross-validation, using many small validation sets. Each model is evaluated once per validation set after it is trained on the rest of the data. By averaging out all the evaluations of a model, you get a much more accurate measure of its performance. There is a drawback, however: the training time is multiplied by the number of validation sets.\\nData Mismatch\\nIn some cases, it’s easy to get a large amount of data for training, but this data probably won’t be perfectly representative of the data that will be used in production. For example, suppose you want to create a mobile app to take pictures of flowers and automatically determine their species. You can easily download millions of pictures of flowers on the web, but they won’t be perfectly representative of the pictures that will actually be taken using the app on a mobile device. Perhaps you only have 10,000 representative pictures (i.e., actually taken with the app). In this case, the most important rule to remember is that the validation set and the test set must be as representative as possible of the data you expect to use in production, so they should be composed exclusively of representative pictures: you can shuffle them and put half in the validation set and half in the test set (making sure that no duplicates or near-duplicates end up in both sets). But after training your model on the web pictures, if you observe that the performance of the model on the validation set is disappointing, you will not know whether this is because your model has overfit the training set, or whether this is just due to the mismatch between the web pictures and the mobile app pictures. One solution is to hold out some of the training pictures (from the web) in yet another set that Andrew Ng calls the train- dev set. After the model is trained (on the training set, not on the train-dev set), you can evaluate it on the train-dev set. If it performs well, then the model is not overfitting the training set. If it performs poorly on the validation set, the problem must be coming from the data mismatch. You can try to tackle this problem by preprocessing the web images to make them look more like the pictures that will be taken by the mobile app, and then retraining the model. Conversely, if the model performs poorly on the train-dev set, then it must have overfit the training set, so you should try to simplify or regularize the model, get more training data, and clean up the training data.NO FREE LUNCH THEOREM\\nA model is a simplified version of the observations. The simplifications are meant to discard the superfluous details that are unlikely to generalize to new instances. To decide what data to discard and what data to keep, you must make assumptions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored.\\nIn a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and evaluate only a few reasonable models. For example, for simple tasks you may evaluate linear models with various levels of regularization, and for a complex problem you may evaluate various neural networks.Welcome to the Machine Learning Housing Corporation! Your first task is to use California census data to build a model of housing prices in the state. This data includes metrics such as the population, median income, and median housing price for each block group in California. Block groups are the smallest geographical unit for which the US Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people). We will call them “districts” for short.\\nYour model should learn from this data and be able to predict the median housing price in any district, given all the other metrics.TIP\\nSince you are a well-organized data scientist, the first thing you should do is pull out your Machine Learning project checklist. You can start with the one in Appendix B; it should work reasonably well for most Machine Learning projects, but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self-explanatory or because they will be discussed in later chapters.\\nThe next question to ask your boss is what the current solution looks like (if any). The current situation will often give you a reference for performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up- to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\\nThis is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%. This is why the company thinks that it would be useful to train a model to predict a district’s median housing price, given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.\\nWith all this information, you are now ready to start designing your system. First, you need to frame the problem: is it supervised, unsupervised, or\\nPIPELINES\\nA sequence of data processing components is called a data pipeline. Pipelines are very common in Machine Learning systems, since there is a lot of data to manipulate and many data transformations to apply.\\nComponents typically run asynchronously. Each component pulls in a large amount of data, processes it, and spits out the result in another data store. Then, some time later, the next component in the pipeline pulls this data and spits out its own output. Each component is fairly self-contained: the interface between components is simply the data store. This makes the system simple to grasp (with the help of a data flow graph), and different teams can focus on different components. Moreover, if a component breaks down, the downstream components can often continue to run normally (at least for a while) by just using the last output from the broken component. This makes the architecture quite robust.\\nOn the other hand, a broken component can go unnoticed for some time if proper monitoring is not implemented. The data gets stale and the overall system’s performance drops.The next question to ask your boss is what the current solution looks like (if any). The current situation will often give you a reference for performance, as well as insights on how to solve the problem. Your boss answers that the district housing prices are currently estimated manually by experts: a team gathers up- to-date information about a district, and when they cannot get the median housing price, they estimate it using complex rules.\\nThis is costly and time-consuming, and their estimates are not great; in cases where they manage to find out the actual median housing price, they often realize that their estimates were off by more than 20%. This is why the company thinks that it would be useful to train a model to predict a district’s median housing price, given other data about that district. The census data looks like a great dataset to exploit for this purpose, since it includes the median housing prices of thousands of districts, as well as other data.\\nWith all this information, you are now ready to start designing your system. First, you need to frame the problem: is it supervised, unsupervised, or Reinforcement Learning? Is it a classification task, a regression task, or something else? Should you use batch learning or online learning techniques? Before you read on, pause and try to answer these questions for yourself.\\nHave you found the answers? Let’s see: it is clearly a typical supervised learning task, since you are given labeled training examples (each instance comes with the expected output, i.e., the district’s median housing price). It is also a typical regression task, since you are asked to predict a value. More specifically, this is a multiple regression problem, since the system will use multiple features to make a prediction (it will use the district’s population, the median income, etc.). It is also a univariate regression problem, since we are only trying to predict a single value for each district. If we were trying to predict multiple values per district, it would be a multivariate regression problem. Finally, there is no continuous flow of data coming into the system, there is no particular need to adjust to changing data rapidly, and the data is small enough to fit in memory, so plain batch learning should do just fine.\\nTIP\\nIf the data were huge, you could either split your batch learning work across multiple servers (using the MapReduce technique) or use an online learning technique.\\nSelect a Performance Measure\\nYour next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.A notebook contains a list of cells. Each cell can contain executable code or formatted text. Right now the notebook contains only one empty code cell, labeled “In [1]:”. Try typing in the cell and clicking the play button (see Figure 2-4) or pressing Shift-Enter. This sends the current cell to this notebook’s Python kernel, which runs it and returns the output. The result is displayed below the cell, and since you’ve reached the end of the notebook, a new cell is automatically created. Go through the User Interface Tour from Jupyter’s Help menu to learn the basics.\\nFigure 2-4. Hello world Python notebook\\nDownload the Data\\n )\"!dlrow olleH\"(tnirp\\nIn typical environments your data would be available in a relational database (or some other common data store) and spread across multiple tables/documents/files. To access it, you would first need to get your credentials and access authorizations10 and familiarize yourself with the data schema. In this project, however, things are much simpler: you will just download a single compressed file, housing.tgz, which contains a comma-separated values (CSV) file called housing.csv with all the data.\\nYou could use your web browser to download the file and run\\nto decompress it and extract the CSV file, but it is preferable to\\ncreate a small function to do that. Having a function that downloads the data is useful in particular if the data changes regularly: you can write a small script that uses the function to fetch the latest data (or you can set up a scheduled job to do that automatically at regular intervals). Automating the process of fetching the data is also useful if you need to install the dataset on multiple machines.\\nHere is the function to fetch the data:11\\n     Now when you call , it creates a datasets/housing directory in your workspace, downloads the housing.tgz file, and extracts the housing.csv file from it in this directory.\\nNow let’s load the data using pandas. Once again, you should write a small function to load the data:housands of dollars (e.g., 3 actually means about $30,000). Working with preprocessed attributes is common in Machine Learning, and it is not necessarily a problem, but you should try to understand how the data was computed.\\n2. The housing median age and the median house value were also capped. The latter may be a serious problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit. You need to check with your client team (the team that will use your system’s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have two options:\\na. Collect proper labels for the districts whose labels were capped.\\nb. Remove those districts from the training set (and also from the test set, since your system should not be evaluated poorly if it predicts values beyond $500,000).\\n3. These attributes have very different scales. We will discuss this later in this chapter, when we explore feature scaling.\\n4. Finally, many histograms are tail-heavy: they extend much farther to the right of the median than to the left. This may make it a bit harder for some Machine Learning algorithms to detect patterns. We will try transforming these attributes later on to have more bell-shaped distributions.Hopefully you now have a better understanding of the kind of data you are dealing with.\\nWARNING\\nWait! Before you look at the data any further, you need to create a test set, put it aside, and never look at it.\\nCreate a Test Set\\nIt may sound strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected. This is called data snooping bias.\\nCreating a test set is theoretically simple: pick some instances randomly, typically 20% of the dataset (or less if your dataset is very large), and set them aside:Well, this works, but it is not perfect: if you run the program again, it will generate a different test set! Over time, you (or your Machine Learning algorithms) will get to see the whole dataset, which is what you want to avoid.\\nOne solution is to save the test set on the first run and then load it in subsequent runs. Another option is to set the random number generator’s seed (e.g., with\\n)14 before calling so that it always generates the same shuffled indices.But both these solutions will break the next time you fetch an updated dataset. To have a stable train/test split even after updating the dataset, a common solution is to use each instance’s identifier to decide whether or not it should go in the test set (assuming instances have a unique and immutable identifier). For example, you could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. This ensures that the test set will remain consistent across multiple runs, even if you refresh the dataset. The new test set will contain 20% of the new instances, but it will not contain any instance that was previously in the training set.Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID If you use the row index as a unique identifier, you need to make sure that new data gets appended to the end of the dataset and that no row ever gets deleted. If this is not possible, then you can try to use the most stable features to build a unique identifier. For example, a district’s latitude and longitude are guaranteed\\nto be stable for a few million years, so you could combine them into an ID like so:15 Scikit-Learn provides a few functions to split datasets into multiple subsets in\\nvarious ways. The simplest function is pretty much the same thing as the function\\ncouple of additional features. First, there is a\\nallows you to set the random generator seed. Second, you can pass it multiple datasets with an identical number of rows, and it will split them on the same indices (this is very useful, for example, if you have a separate DataFrame for labels):\\nSo far we have considered purely random sampling methods. This is generally fine if your dataset is large enough (especially relative to the number of attributes), but if it is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don’t just pick 1,000 people randomly in a phone book. They try to ensure that these 1,000 people are representative of the whole population. For example, the US population is 51.3% females and 48.7% males, so a well- conducted survey in the US would try to maintain this ratio in the sample: 513 female and 487 male. This is called stratified sampling: the population is divided into homogeneous subgroups called strata, and the right number of instances are sampled from each stratum to guarantee that the test set is representative of the overall population. If the people running the survey used purely random sampling, there would be about a 12% chance of sampling a skewed test set that was either less than 49% female or more than 54% female. Either way, the survey results would be significantly biased.\\nSuppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices. You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset. Since the median income is a continuous numerical attribute, you first need to create an income category attribute. Let’s look at the median income histogram more closely (back in Figure 2-8): most median income values are clustered around 1.5 to 6 (i.e., $15,000–$60,000), but some median incomes go far beyond 6. It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of a stratum’s importance may be biased. This means that you should not have too many strata, and each stratum should be large enough. The following code uses the\\nfunction to create an income category attribute with five categories (labeled from 1 to 5): category 1 ranges from 0 to 1.5 (i.e., less than $15,000), category 2 from 1.5 to 3, and so on: With similar code you can measure the income category proportions in the full dataset. Figure 2-10 compares the income category proportions in the overall dataset, in the test set generated with stratified sampling, and in a test set generated using purely random sampling. As you can see, the test set generated using stratified sampling has income category proportions almost identical to those in the full dataset, whereas the test set generated using purely random sampling is skewed.We spent quite a bit of time on test set generation for a good reason: this is an often neglected but critical part of a Machine Learning project. Moreover, many of these ideas will be useful later when we discuss cross-validation. Now it’s time to move on to the next stage: exploring the data.So far you have only taken a quick glance at the data to get a general understanding of the kind of data you are manipulating. Now the goal is to go into a little more depth.\\nFirst, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast. In our case, the set is quite small, so you can just work directly on the full set. Let’s create a copy so that you can play with it without harming the training set: Now that’s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high density in the Central Valley, in particular around Sacramento and Fresno.\\nOur brains are very good at spotting patterns in pictures, but you may need to play around with visualization parameters to make the patterns stand out.\\nNow let’s look at the housing prices (Figure 2-13). The radius of each circle represents the district’s population (option ), and the color represents the price (option ). We will use a predefined color map (option ) called , which ranges from blue (low values) to red (high prices):16 This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) and to the population density, as you probably knew already. A clustering algorithm should be useful for detecting the main cluster and for adding new features that measure the proximity to the cluster centers. The ocean proximity attribute may be useful as well, although in Northern California the housing prices in coastal districts are not too high, so it is not a simple rule.\\nLooking for Correlations\\nSince the dataset is not too large, you can easily compute the standard correlation coefficient (also called Pearson’s r) between every pair of attributes using the method: The correlation coefficient ranges from –1 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up. When the coefficient is close to –1, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to 0 mean that there is no linear correlation. Figure 2-14 shows various plots along with the correlation coefficient between their horizontal and vertical axes.Another way to check for correlation between attributes is to use the pandas function, which plots every numerical attribute against\\nevery other numerical attribute. Since there are now 11 numerical attributes, you would get 112 = 121 plots, which would not fit on a page—so let’s just focus on a few promising attributes that seem most correlated with the median housing value (Figure 2-15): The main diagonal (top left to bottom right) would be full of straight lines if pandas plotted each variable against itself, which would not be very useful. So instead pandas displays a histogram of each attribute (other options are available; see the pandas documentation for more details).\\nThe most promising attribute to predict the median house value is the median income, so let’s zoom in on their correlation scatterplot (Figure 2-16): his plot reveals a few things. First, the correlation is indeed very strong; you can clearly see the upward trend, and the points are not too dispersed. Second, the price cap that we noticed earlier is clearly visible as a horizontal line at $500,000. But this plot reveals other less obvious straight lines: a horizontal line around $450,000, another around $350,000, perhaps one around $280,000, and a few more below that. You may want to try removing the corresponding districts to prevent your algorithms from learning to reproduce these data quirks.\\nExperimenting with Attribute Combinations\\nHopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights. You identified a few data quirks that you may want to clean up before feeding the data to a Machine Learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute. You also noticed that some attributes have a tail-heavy distribution, so you may want to transform them (e.g., by computing their logarithm). Of course, your mileage will vary considerably with each project, but the general ideas are similar.One last thing you may want to do before preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don’t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at. Let’s create these new attributes: Hey, not bad! The new attribute is much more correlated with the median house value than the total number of rooms or bedrooms. Apparently houses with a lower bedroom/room ratio tend to be more expensive. The number of rooms per household is also more informative than the total number of rooms in a district—obviously the larger the houses, the more expensive they are.\\nThis round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first reasonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.\\nPrepare the Data for Machine Learning Algorithms\\nIt’s time to prepare the data for your Machine Learning algorithms. Instead of doing this manually, you should write functions for this purpose, for several good reasons:\\nThis will allow you to reproduce these transformations easily on any dataset (e.g., the next time you get a fresh dataset).\\nYou will gradually build a library of transformation functions that you can reuse in future projects.\\nYou can use these functions in your live system to transform the new data before feeding it to your algorithms.\\nThis will make it possible for you to easily try various transformations and see which combination of transformations works best.\\nBut first let’s revert to a clean training set (by copying once again). Let’s also separate the predictors and the labels, since we don’t necessarily want to apply the same transformations to the predictors and the target values (note that creates a copy of the data and does not affect\\n):\\nData Cleaning\\nMost Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. We saw earlier that the\\nthree options:  Scikit-Learn’s API is remarkably well designed. These are the main design principles:17\\nConsistency\\nAll objects share a consistent and simple interface: Estimators\\nAny object that can estimate some parameters based on a dataset is called an estimator (e.g., an is an estimator). The\\nestimation itself is performed by the method, and it takes only a dataset as a parameter (or two for supervised learning algorithms; the second dataset contains the labels). Any other parameter needed to guide the estimation process is considered a hyperparameter (such as an ’s ), and it must be set as an instance variable (generally via a constructor parameter).\\nTransformers\\nSome estimators (such as an ) can also transform a dataset; these are called transformers. Once again, the API is simple: the transformation is performed by the method with the dataset to transform as a parameter. It returns the transformed dataset. This transformation generally relies on the learned\\nparameters, as is the case for an a convenience method called calling and then\\nPredictors\\nFinally, some estimators, given a dataset, are capable of making predictions; they are called predictors. For example, the\\nmodel in the previous chapter was a predictor: given a country’s GDP per capita, it predicted life satisfaction. A\\npredictor has a method that takes a dataset of new instances and returns a dataset of corresponding predictions. It also has a method that measures the quality of the predictions, given a test set (and the corresponding labels, in the case of supervised learning algorithms).18\\nInspection\\nAll the estimator’s hyperparameters are accessible directly via public instance variables (e.g., ), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., ).\\nNonproliferation of classes\\nDatasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers.\\nComposition\\nExisting building blocks are reused as much as possible. For example, it is easy to create a estimator from an arbitrary sequence of transformers followed by a final estimator, as we will see.\\nSensible defaults\\nScikit-Learn provides reasonable default values for most parameters, making it easy to quickly create a baseline working system.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " '-': 1,\n",
       " 'Z': 2,\n",
       " 'S': 3,\n",
       " 'y': 4,\n",
       " '/': 5,\n",
       " ' ': 6,\n",
       " 'θ': 7,\n",
       " 'V': 8,\n",
       " '4': 9,\n",
       " '0': 10,\n",
       " '!': 11,\n",
       " ',': 12,\n",
       " 'j': 13,\n",
       " 'A': 14,\n",
       " ';': 15,\n",
       " 'D': 16,\n",
       " 't': 17,\n",
       " '’': 18,\n",
       " 'g': 19,\n",
       " 'T': 20,\n",
       " 'w': 21,\n",
       " 'l': 22,\n",
       " 'd': 23,\n",
       " 'n': 24,\n",
       " '1': 25,\n",
       " 'F': 26,\n",
       " 'f': 27,\n",
       " 'X': 28,\n",
       " 'R': 29,\n",
       " '+': 30,\n",
       " '$': 31,\n",
       " '”': 32,\n",
       " 'z': 33,\n",
       " 'C': 34,\n",
       " 'k': 35,\n",
       " 'c': 36,\n",
       " 'o': 37,\n",
       " '(': 38,\n",
       " '8': 39,\n",
       " '.': 40,\n",
       " ':': 41,\n",
       " 'W': 42,\n",
       " 'a': 43,\n",
       " 'N': 44,\n",
       " 'i': 45,\n",
       " ')': 46,\n",
       " 'B': 47,\n",
       " '=': 48,\n",
       " 'E': 49,\n",
       " '%': 50,\n",
       " '×': 51,\n",
       " 'U': 52,\n",
       " '?': 53,\n",
       " 'Y': 54,\n",
       " 'v': 55,\n",
       " ']': 56,\n",
       " '\\n': 57,\n",
       " 'Q': 58,\n",
       " 'u': 59,\n",
       " '_': 60,\n",
       " 'm': 61,\n",
       " 'p': 62,\n",
       " '7': 63,\n",
       " '3': 64,\n",
       " '6': 65,\n",
       " 'r': 66,\n",
       " 'q': 67,\n",
       " 'h': 68,\n",
       " '2': 69,\n",
       " 'I': 70,\n",
       " 'O': 71,\n",
       " '[': 72,\n",
       " '“': 73,\n",
       " 'G': 74,\n",
       " 'K': 75,\n",
       " 'e': 76,\n",
       " '–': 77,\n",
       " 's': 78,\n",
       " 'x': 79,\n",
       " 'H': 80,\n",
       " 'J': 81,\n",
       " '\"': 82,\n",
       " '—': 83,\n",
       " '9': 84,\n",
       " 'M': 85,\n",
       " 'L': 86,\n",
       " 'b': 87,\n",
       " '5': 88}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 68, 76, ..., 76, 61, 40], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'When most people hear “Machine Learning,” they picture a robot: a dependable butler or a deadly Term'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check first 100 words from file\n",
    "\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 68, 76, 24,  6, 61, 37, 78, 17,  6, 62, 76, 37, 62, 22, 76,  6,\n",
       "       68, 76, 43, 66,  6, 73, 85, 43, 36, 68, 45, 24, 76,  6, 86, 76, 43,\n",
       "       66, 24, 45, 24, 19, 12, 32,  6, 17, 68, 76,  4,  6, 62, 45, 36, 17,\n",
       "       59, 66, 76,  6, 43,  6, 66, 37, 87, 37, 17, 41,  6, 43,  6, 23, 76,\n",
       "       62, 76, 24, 23, 43, 87, 22, 76,  6, 87, 59, 17, 22, 76, 66,  6, 37,\n",
       "       66,  6, 43,  6, 23, 76, 43, 23, 22,  4,  6, 20, 76, 66, 61],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the corresponding numerical variable\n",
    "\n",
    "char[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check the maximum number of class available inside file\n",
    "\n",
    "np.max(char)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating function to split data\n",
    "\n",
    "def split_data(char,batch_size,num_step,split_frac=0.9):\n",
    "    \n",
    "    slice_size = batch_size*num_step\n",
    "    n_batches = int(len(char)/slice_size)\n",
    "    \n",
    "    \n",
    "    x = char[:n_batches*slice_size]\n",
    "    y = char[1:n_batches*slice_size+1]\n",
    "    \n",
    "    x = np.stack(np.split(x,batch_size))\n",
    "    y = np.stack(np.split(y,batch_size))\n",
    "    \n",
    "    split_idx = int(n_batches*split_frac)\n",
    "    train_x,train_y = x[:, :split_idx*num_step],y[:,:split_idx*num_step]\n",
    "    test_x,test_y = x[:,split_idx*num_step:],y[:,split_idx*num_step:]\n",
    "    \n",
    "    return train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset\n",
    "\n",
    "train_x,train_y,test_x,test_y = split_data(char,10,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 6400)\n",
      "(10, 6400)\n",
      "(10, 750)\n",
      "(10, 750)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, 68, 76, 24,  6, 61, 37, 78, 17,  6, 62, 76, 37, 62, 22, 76,\n",
       "         6, 68, 76, 43, 66,  6, 73, 85, 43, 36, 68, 45, 24, 76,  6, 86,\n",
       "        76, 43, 66, 24, 45, 24, 19, 12, 32,  6, 17, 68, 76,  4,  6, 62,\n",
       "        45, 36],\n",
       "       [76, 78,  6, 37, 27,  6, 17, 76, 36, 68, 24, 45, 67, 59, 76, 78,\n",
       "         6, 17, 68, 43, 17,  6,  4, 37, 59,  6, 21, 37, 59, 22, 23,  6,\n",
       "        59, 78, 76,  6, 27, 37, 66,  6, 76, 43, 36, 68,  6, 17, 43, 78,\n",
       "        35, 40],\n",
       "       [22, 76,  6, 78,  4, 78, 17, 76, 61,  6, 45, 78,  6, 27, 45, 24,\n",
       "        76,  1, 17, 59, 24, 76, 23,  6, 59, 78, 45, 24, 19,  6, 78, 59,\n",
       "        62, 76, 66, 55, 45, 78, 76, 23,  6, 22, 76, 43, 66, 24, 45, 24,\n",
       "        19,  6],\n",
       "       [68, 76,  6, 78,  4, 78, 17, 76, 61,  6, 21, 37, 59, 22, 23,  6,\n",
       "        27, 22, 43, 19,  6, 43, 24,  6, 76, 61, 43, 45, 22,  6, 43, 78,\n",
       "         6, 78, 62, 43, 61,  6, 45, 27,  6, 45, 17,  6, 68, 43, 78,  6,\n",
       "        61, 43],\n",
       "       [76, 78,  6, 78, 76, 76, 61,  6, 68, 43, 62, 62, 45, 76, 66,  6,\n",
       "        17, 68, 43, 24,  6, 61, 43, 24,  4,  6, 66, 45, 36, 68,  6, 36,\n",
       "        37, 59, 24, 17, 66, 45, 76, 78, 40, 47,  4,  6, 59, 78, 45, 24,\n",
       "        19,  6],\n",
       "       [68, 37, 21, 78,  6, 17, 68, 66, 76, 76,  6, 61, 37, 23, 76, 22,\n",
       "        78, 40,  6, 20, 68, 76,  6, 23, 37, 17, 17, 76, 23,  6, 22, 45,\n",
       "        24, 76,  6, 66, 76, 62, 66, 76, 78, 76, 24, 17, 78,  6, 17, 68,\n",
       "        76,  6],\n",
       "       [19,  6, 78, 76, 17, 40,  6, 70, 17,  6, 21, 37, 59, 22, 23,  6,\n",
       "        87, 76,  6, 22, 45, 35, 76,  6, 78, 76, 22, 76, 36, 17, 45, 24,\n",
       "        19,  6, 17, 68, 76,  6, 27, 43, 78, 17, 76, 78, 17,  6, 78, 62,\n",
       "        66, 45],\n",
       "       [61, 18, 78,  6, 62, 76, 66, 27, 37, 66, 61, 43, 24, 36, 76,  6,\n",
       "        23, 66, 37, 62, 78, 40, 20, 68, 76,  6, 24, 76, 79, 17,  6, 67,\n",
       "        59, 76, 78, 17, 45, 37, 24,  6, 17, 37,  6, 43, 78, 35,  6,  4,\n",
       "        37, 59],\n",
       "       [45, 43, 78, 40, 57, 34, 66, 76, 43, 17, 45, 24, 19,  6, 43,  6,\n",
       "        17, 76, 78, 17,  6, 78, 76, 17,  6, 45, 78,  6, 17, 68, 76, 37,\n",
       "        66, 76, 17, 45, 36, 43, 22, 22,  4,  6, 78, 45, 61, 62, 22, 76,\n",
       "        41,  6],\n",
       "       [36, 37, 76, 27, 27, 45, 36, 45, 76, 24, 17,  6, 38, 43, 22, 78,\n",
       "        37,  6, 36, 43, 22, 22, 76, 23,  6,  0, 76, 43, 66, 78, 37, 24,\n",
       "        18, 78,  6, 66, 46,  6, 87, 76, 17, 21, 76, 76, 24,  6, 76, 55,\n",
       "        76, 66]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check train dataset\n",
    "\n",
    "train_x[:,:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating model function\n",
    "\n",
    "def build_rnn(num_classes,batch_size=50,num_step=50,lstm_size=128,num_layers=2,learning_rate = 0.001,\n",
    "       grad_clip=5,sampling=False):\n",
    "    \n",
    "    if sampling==True:\n",
    "        batch_size,num_step=1,1\n",
    "        \n",
    "    #tf.reset_default_graph()\n",
    "    tf.reset_default_graph()\n",
    "    #tf.disable_v2_behavior() \n",
    "    \n",
    "    \n",
    "    inputs = tf.placeholder(tf.int32,[batch_size,num_step],name = 'inputs')\n",
    "    targets = tf.placeholder(tf.int32,[batch_size,num_step],name = 'targets')\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32,name = 'keep_prob')\n",
    "    \n",
    "    x_one_hot = tf.one_hot(inputs,num_classes)\n",
    "    y_one_hot = tf.one_hot(targets,num_classes)\n",
    "    \n",
    "    # build LSTM\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # dropout ratio\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,output_keep_prob = keep_prob)\n",
    "    \n",
    "    # creating multiple layer using multi layer\n",
    "    cell = tf.contrib.rnn.MultiRNNCell([drop],num_layers)\n",
    "    initial_state = cell.zero_state(batch_size,tf.float32)\n",
    "    \n",
    "    rnn_inputs = [tf.squeeze(i,squeeze_dims=[1]) for i in tf.split(x_one_hot,num_step,1)]\n",
    "    \n",
    "    outputs,state = tf.contrib.rnn.static_rnn(cell,rnn_inputs,initial_state = initial_state)\n",
    "    final_state = state\n",
    "    \n",
    "    seq_output = tf.concat(outputs,axis = 1)\n",
    "    output = tf.reshape(seq_output,[-1,lstm_size])\n",
    "    \n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal((lstm_size,num_classes),stddev = 0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "        \n",
    "    logits = tf.matmul(output,softmax_w) + softmax_b\n",
    "    \n",
    "    preds = tf.nn.softmax(logits,name = 'predictions')\n",
    "    \n",
    "    y_reshaped = tf.reshape(y_one_hot,[-1,num_classes])\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels = y_reshaped)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "    \n",
    "    # optimizer\n",
    "    tvars = tf.trainable_variables()\n",
    "    grads,_ = tf.clip_by_global_norm(tf.gradients(cost,tvars),grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads,tvars))\n",
    "    \n",
    "    export_nodes = ['inputs','targets','initial_state','final_state','keep_prob','cost','preds','optimizer']\n",
    "    Graph = namedtuple('Graph',export_nodes)\n",
    "    local_dict = locals()\n",
    "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "num_step = 100 \n",
    "lstm_size = 512\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "keep_prob = 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: checkpoints: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 35\n",
    "# Save every N iterations\n",
    "save_every_n = 200\n",
    "train_x, train_y, test_x, test_y = split_data(char, batch_size, num_step)\n",
    "\n",
    "model = build_rnn(len(vocab), \n",
    "                  batch_size=batch_size,\n",
    "                  num_step=num_step,\n",
    "                  learning_rate=learning_rate,\n",
    "                  lstm_size=lstm_size,\n",
    "                  num_layers=num_layers)\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(arrs, num_step):\n",
    "    batch_size, slice_size = arrs[0].shape\n",
    "    \n",
    "    n_batches = int(slice_size/num_step)\n",
    "    for b in range(n_batches):\n",
    "        yield [x[:, b*num_step: (b+1)*num_step] for x in arrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35  Iteration 1/210 Training loss: 4.4836 2.1586 sec/batch\n",
      "Epoch 1/35  Iteration 2/210 Training loss: 4.4527 1.7658 sec/batch\n",
      "Epoch 1/35  Iteration 3/210 Training loss: 4.3923 1.7703 sec/batch\n",
      "Epoch 1/35  Iteration 4/210 Training loss: 4.2873 1.8190 sec/batch\n",
      "Epoch 1/35  Iteration 5/210 Training loss: 4.0989 1.8583 sec/batch\n",
      "Epoch 1/35  Iteration 6/210 Training loss: 3.9809 1.7911 sec/batch\n",
      "Epoch 2/35  Iteration 7/210 Training loss: 3.5751 1.7600 sec/batch\n",
      "Epoch 2/35  Iteration 8/210 Training loss: 3.4280 1.7937 sec/batch\n",
      "Epoch 2/35  Iteration 9/210 Training loss: 3.3412 1.7612 sec/batch\n",
      "Epoch 2/35  Iteration 10/210 Training loss: 3.2861 1.8183 sec/batch\n",
      "Epoch 2/35  Iteration 11/210 Training loss: 3.2590 1.8558 sec/batch\n",
      "Epoch 2/35  Iteration 12/210 Training loss: 3.2395 1.7824 sec/batch\n",
      "Epoch 3/35  Iteration 13/210 Training loss: 3.2422 1.7786 sec/batch\n",
      "Epoch 3/35  Iteration 14/210 Training loss: 3.1576 1.7723 sec/batch\n",
      "Epoch 3/35  Iteration 15/210 Training loss: 3.1312 1.7708 sec/batch\n",
      "Epoch 3/35  Iteration 16/210 Training loss: 3.1211 1.7970 sec/batch\n",
      "Epoch 3/35  Iteration 17/210 Training loss: 3.1156 1.7593 sec/batch\n",
      "Epoch 3/35  Iteration 18/210 Training loss: 3.1088 1.8351 sec/batch\n",
      "Epoch 4/35  Iteration 19/210 Training loss: 3.1763 1.7860 sec/batch\n",
      "Epoch 4/35  Iteration 20/210 Training loss: 3.1155 1.7630 sec/batch\n",
      "Epoch 4/35  Iteration 21/210 Training loss: 3.0950 1.7599 sec/batch\n",
      "Epoch 4/35  Iteration 22/210 Training loss: 3.0881 1.8201 sec/batch\n",
      "Epoch 4/35  Iteration 23/210 Training loss: 3.0837 1.7668 sec/batch\n",
      "Epoch 4/35  Iteration 24/210 Training loss: 3.0779 1.8587 sec/batch\n",
      "Epoch 5/35  Iteration 25/210 Training loss: 3.1476 1.7750 sec/batch\n",
      "Epoch 5/35  Iteration 26/210 Training loss: 3.0900 1.7590 sec/batch\n",
      "Epoch 5/35  Iteration 27/210 Training loss: 3.0741 1.7791 sec/batch\n",
      "Epoch 5/35  Iteration 28/210 Training loss: 3.0687 1.8239 sec/batch\n",
      "Epoch 5/35  Iteration 29/210 Training loss: 3.0654 1.7763 sec/batch\n",
      "Epoch 5/35  Iteration 30/210 Training loss: 3.0599 1.8566 sec/batch\n",
      "Epoch 6/35  Iteration 31/210 Training loss: 3.1249 1.7760 sec/batch\n",
      "Epoch 6/35  Iteration 32/210 Training loss: 3.0727 1.7622 sec/batch\n",
      "Epoch 6/35  Iteration 33/210 Training loss: 3.0578 1.7939 sec/batch\n",
      "Epoch 6/35  Iteration 34/210 Training loss: 3.0537 2.1348 sec/batch\n",
      "Epoch 6/35  Iteration 35/210 Training loss: 3.0514 1.7762 sec/batch\n",
      "Epoch 6/35  Iteration 36/210 Training loss: 3.0463 1.8303 sec/batch\n",
      "Epoch 7/35  Iteration 37/210 Training loss: 3.1084 1.7671 sec/batch\n",
      "Epoch 7/35  Iteration 38/210 Training loss: 3.0568 1.7965 sec/batch\n",
      "Epoch 7/35  Iteration 39/210 Training loss: 3.0424 1.8145 sec/batch\n",
      "Epoch 7/35  Iteration 40/210 Training loss: 3.0390 1.7681 sec/batch\n",
      "Epoch 7/35  Iteration 41/210 Training loss: 3.0372 1.8134 sec/batch\n",
      "Epoch 7/35  Iteration 42/210 Training loss: 3.0327 1.8563 sec/batch\n",
      "Epoch 8/35  Iteration 43/210 Training loss: 3.0956 1.7816 sec/batch\n",
      "Epoch 8/35  Iteration 44/210 Training loss: 3.0444 1.7719 sec/batch\n",
      "Epoch 8/35  Iteration 45/210 Training loss: 3.0301 1.7781 sec/batch\n",
      "Epoch 8/35  Iteration 46/210 Training loss: 3.0268 1.7791 sec/batch\n",
      "Epoch 8/35  Iteration 47/210 Training loss: 3.0249 2.1834 sec/batch\n",
      "Epoch 8/35  Iteration 48/210 Training loss: 3.0200 1.9003 sec/batch\n",
      "Epoch 9/35  Iteration 49/210 Training loss: 3.0796 1.7819 sec/batch\n",
      "Epoch 9/35  Iteration 50/210 Training loss: 3.0304 1.9644 sec/batch\n",
      "Epoch 9/35  Iteration 51/210 Training loss: 3.0168 1.8024 sec/batch\n",
      "Epoch 9/35  Iteration 52/210 Training loss: 3.0135 2.1828 sec/batch\n",
      "Epoch 9/35  Iteration 53/210 Training loss: 3.0112 1.7776 sec/batch\n",
      "Epoch 9/35  Iteration 54/210 Training loss: 3.0063 1.7834 sec/batch\n",
      "Epoch 10/35  Iteration 55/210 Training loss: 3.0633 1.8356 sec/batch\n",
      "Epoch 10/35  Iteration 56/210 Training loss: 3.0142 2.0435 sec/batch\n",
      "Epoch 10/35  Iteration 57/210 Training loss: 3.0013 2.1383 sec/batch\n",
      "Epoch 10/35  Iteration 58/210 Training loss: 2.9983 2.3273 sec/batch\n",
      "Epoch 10/35  Iteration 59/210 Training loss: 2.9961 1.9936 sec/batch\n",
      "Epoch 10/35  Iteration 60/210 Training loss: 2.9908 2.2569 sec/batch\n",
      "Epoch 11/35  Iteration 61/210 Training loss: 3.0459 1.7902 sec/batch\n",
      "Epoch 11/35  Iteration 62/210 Training loss: 2.9972 1.7909 sec/batch\n",
      "Epoch 11/35  Iteration 63/210 Training loss: 2.9843 1.8564 sec/batch\n",
      "Epoch 11/35  Iteration 64/210 Training loss: 2.9811 1.8202 sec/batch\n",
      "Epoch 11/35  Iteration 65/210 Training loss: 2.9785 1.9689 sec/batch\n",
      "Epoch 11/35  Iteration 66/210 Training loss: 2.9730 1.9038 sec/batch\n",
      "Epoch 12/35  Iteration 67/210 Training loss: 3.0266 1.8342 sec/batch\n",
      "Epoch 12/35  Iteration 68/210 Training loss: 2.9771 1.8664 sec/batch\n",
      "Epoch 12/35  Iteration 69/210 Training loss: 2.9644 1.7939 sec/batch\n",
      "Epoch 12/35  Iteration 70/210 Training loss: 2.9609 1.8254 sec/batch\n",
      "Epoch 12/35  Iteration 71/210 Training loss: 2.9580 1.7599 sec/batch\n",
      "Epoch 12/35  Iteration 72/210 Training loss: 2.9522 1.7814 sec/batch\n",
      "Epoch 13/35  Iteration 73/210 Training loss: 3.0004 1.8997 sec/batch\n",
      "Epoch 13/35  Iteration 74/210 Training loss: 2.9536 2.2946 sec/batch\n",
      "Epoch 13/35  Iteration 75/210 Training loss: 2.9412 1.9509 sec/batch\n",
      "Epoch 13/35  Iteration 76/210 Training loss: 2.9376 1.8083 sec/batch\n",
      "Epoch 13/35  Iteration 77/210 Training loss: 2.9344 1.7793 sec/batch\n",
      "Epoch 13/35  Iteration 78/210 Training loss: 2.9279 1.8117 sec/batch\n",
      "Epoch 14/35  Iteration 79/210 Training loss: 2.9727 1.8452 sec/batch\n",
      "Epoch 14/35  Iteration 80/210 Training loss: 2.9257 1.8133 sec/batch\n",
      "Epoch 14/35  Iteration 81/210 Training loss: 2.9131 1.7888 sec/batch\n",
      "Epoch 14/35  Iteration 82/210 Training loss: 2.9094 1.7549 sec/batch\n",
      "Epoch 14/35  Iteration 83/210 Training loss: 2.9060 1.7722 sec/batch\n",
      "Epoch 14/35  Iteration 84/210 Training loss: 2.8988 1.8036 sec/batch\n",
      "Epoch 15/35  Iteration 85/210 Training loss: 2.9403 1.8756 sec/batch\n",
      "Epoch 15/35  Iteration 86/210 Training loss: 2.8933 1.8014 sec/batch\n",
      "Epoch 15/35  Iteration 87/210 Training loss: 2.8798 1.7812 sec/batch\n",
      "Epoch 15/35  Iteration 88/210 Training loss: 2.8760 1.7748 sec/batch\n",
      "Epoch 15/35  Iteration 89/210 Training loss: 2.8733 1.8051 sec/batch\n",
      "Epoch 15/35  Iteration 90/210 Training loss: 2.8672 1.7695 sec/batch\n",
      "Epoch 16/35  Iteration 91/210 Training loss: 2.9148 1.7965 sec/batch\n",
      "Epoch 16/35  Iteration 92/210 Training loss: 2.8643 1.8345 sec/batch\n",
      "Epoch 16/35  Iteration 93/210 Training loss: 2.8499 1.7616 sec/batch\n",
      "Epoch 16/35  Iteration 94/210 Training loss: 2.8448 1.7691 sec/batch\n",
      "Epoch 16/35  Iteration 95/210 Training loss: 2.8403 1.7830 sec/batch\n",
      "Epoch 16/35  Iteration 96/210 Training loss: 2.8320 1.7877 sec/batch\n",
      "Epoch 17/35  Iteration 97/210 Training loss: 2.8672 1.7772 sec/batch\n",
      "Epoch 17/35  Iteration 98/210 Training loss: 2.8182 1.8715 sec/batch\n",
      "Epoch 17/35  Iteration 99/210 Training loss: 2.8044 1.7707 sec/batch\n",
      "Epoch 17/35  Iteration 100/210 Training loss: 2.8012 1.7703 sec/batch\n",
      "Epoch 17/35  Iteration 101/210 Training loss: 2.7957 1.7978 sec/batch\n",
      "Epoch 17/35  Iteration 102/210 Training loss: 2.7867 1.8005 sec/batch\n",
      "Epoch 18/35  Iteration 103/210 Training loss: 2.8257 1.8091 sec/batch\n",
      "Epoch 18/35  Iteration 104/210 Training loss: 2.7740 1.8597 sec/batch\n",
      "Epoch 18/35  Iteration 105/210 Training loss: 2.7591 1.7748 sec/batch\n",
      "Epoch 18/35  Iteration 106/210 Training loss: 2.7578 1.7882 sec/batch\n",
      "Epoch 18/35  Iteration 107/210 Training loss: 2.7537 1.8523 sec/batch\n",
      "Epoch 18/35  Iteration 108/210 Training loss: 2.7464 1.8027 sec/batch\n",
      "Epoch 19/35  Iteration 109/210 Training loss: 2.7927 1.8601 sec/batch\n",
      "Epoch 19/35  Iteration 110/210 Training loss: 2.7360 1.7580 sec/batch\n",
      "Epoch 19/35  Iteration 111/210 Training loss: 2.7227 1.7653 sec/batch\n",
      "Epoch 19/35  Iteration 112/210 Training loss: 2.7190 1.8068 sec/batch\n",
      "Epoch 19/35  Iteration 113/210 Training loss: 2.7130 1.7889 sec/batch\n",
      "Epoch 19/35  Iteration 114/210 Training loss: 2.7028 1.8058 sec/batch\n",
      "Epoch 20/35  Iteration 115/210 Training loss: 2.7401 1.7813 sec/batch\n",
      "Epoch 20/35  Iteration 116/210 Training loss: 2.6912 1.8294 sec/batch\n",
      "Epoch 20/35  Iteration 117/210 Training loss: 2.6775 1.7797 sec/batch\n",
      "Epoch 20/35  Iteration 118/210 Training loss: 2.6777 1.7877 sec/batch\n",
      "Epoch 20/35  Iteration 119/210 Training loss: 2.6815 1.8340 sec/batch\n",
      "Epoch 20/35  Iteration 120/210 Training loss: 2.6846 1.8051 sec/batch\n",
      "Epoch 21/35  Iteration 121/210 Training loss: 2.7218 1.8536 sec/batch\n",
      "Epoch 21/35  Iteration 122/210 Training loss: 2.6901 1.7604 sec/batch\n",
      "Epoch 21/35  Iteration 123/210 Training loss: 2.6741 1.7801 sec/batch\n",
      "Epoch 21/35  Iteration 124/210 Training loss: 2.6699 1.7817 sec/batch\n",
      "Epoch 21/35  Iteration 125/210 Training loss: 2.6624 1.7903 sec/batch\n",
      "Epoch 21/35  Iteration 126/210 Training loss: 2.6518 1.7720 sec/batch\n",
      "Epoch 22/35  Iteration 127/210 Training loss: 2.6945 1.9047 sec/batch\n",
      "Epoch 22/35  Iteration 128/210 Training loss: 2.6422 1.7884 sec/batch\n",
      "Epoch 22/35  Iteration 129/210 Training loss: 2.6271 1.7880 sec/batch\n",
      "Epoch 22/35  Iteration 130/210 Training loss: 2.6238 1.7936 sec/batch\n",
      "Epoch 22/35  Iteration 131/210 Training loss: 2.6164 1.8906 sec/batch\n",
      "Epoch 22/35  Iteration 132/210 Training loss: 2.6063 1.7653 sec/batch\n",
      "Epoch 23/35  Iteration 133/210 Training loss: 2.6499 1.7806 sec/batch\n",
      "Epoch 23/35  Iteration 134/210 Training loss: 2.5971 1.7930 sec/batch\n",
      "Epoch 23/35  Iteration 135/210 Training loss: 2.5833 1.7697 sec/batch\n",
      "Epoch 23/35  Iteration 136/210 Training loss: 2.5821 1.8120 sec/batch\n",
      "Epoch 23/35  Iteration 137/210 Training loss: 2.5767 1.8797 sec/batch\n",
      "Epoch 23/35  Iteration 138/210 Training loss: 2.5659 1.7629 sec/batch\n",
      "Epoch 24/35  Iteration 139/210 Training loss: 2.6075 1.7952 sec/batch\n",
      "Epoch 24/35  Iteration 140/210 Training loss: 2.5610 1.7619 sec/batch\n",
      "Epoch 24/35  Iteration 141/210 Training loss: 2.5495 1.7993 sec/batch\n",
      "Epoch 24/35  Iteration 142/210 Training loss: 2.5527 1.8340 sec/batch\n",
      "Epoch 24/35  Iteration 143/210 Training loss: 2.5502 1.7791 sec/batch\n",
      "Epoch 24/35  Iteration 144/210 Training loss: 2.5383 1.7687 sec/batch\n",
      "Epoch 25/35  Iteration 145/210 Training loss: 2.5797 1.8753 sec/batch\n",
      "Epoch 25/35  Iteration 146/210 Training loss: 2.5330 1.7620 sec/batch\n",
      "Epoch 25/35  Iteration 147/210 Training loss: 2.5196 1.8026 sec/batch\n",
      "Epoch 25/35  Iteration 148/210 Training loss: 2.5195 1.7664 sec/batch\n",
      "Epoch 25/35  Iteration 149/210 Training loss: 2.5130 1.7858 sec/batch\n",
      "Epoch 25/35  Iteration 150/210 Training loss: 2.5026 1.7573 sec/batch\n",
      "Epoch 26/35  Iteration 151/210 Training loss: 2.5430 1.8604 sec/batch\n",
      "Epoch 26/35  Iteration 152/210 Training loss: 2.4970 1.7690 sec/batch\n",
      "Epoch 26/35  Iteration 153/210 Training loss: 2.4864 1.8289 sec/batch\n",
      "Epoch 26/35  Iteration 154/210 Training loss: 2.4858 1.7710 sec/batch\n",
      "Epoch 26/35  Iteration 155/210 Training loss: 2.4813 1.7761 sec/batch\n",
      "Epoch 26/35  Iteration 156/210 Training loss: 2.4714 1.9040 sec/batch\n",
      "Epoch 27/35  Iteration 157/210 Training loss: 2.5128 1.7947 sec/batch\n",
      "Epoch 27/35  Iteration 158/210 Training loss: 2.4698 1.7966 sec/batch\n",
      "Epoch 27/35  Iteration 159/210 Training loss: 2.4688 1.7837 sec/batch\n",
      "Epoch 27/35  Iteration 160/210 Training loss: 2.4859 1.7684 sec/batch\n",
      "Epoch 27/35  Iteration 161/210 Training loss: 2.4887 1.7978 sec/batch\n",
      "Epoch 27/35  Iteration 162/210 Training loss: 2.4798 1.8174 sec/batch\n",
      "Epoch 28/35  Iteration 163/210 Training loss: 2.5195 1.8074 sec/batch\n",
      "Epoch 28/35  Iteration 164/210 Training loss: 2.4749 1.7885 sec/batch\n",
      "Epoch 28/35  Iteration 165/210 Training loss: 2.4624 1.8024 sec/batch\n",
      "Epoch 28/35  Iteration 166/210 Training loss: 2.4652 1.7830 sec/batch\n",
      "Epoch 28/35  Iteration 167/210 Training loss: 2.4585 1.7889 sec/batch\n",
      "Epoch 28/35  Iteration 168/210 Training loss: 2.4491 1.8134 sec/batch\n",
      "Epoch 29/35  Iteration 169/210 Training loss: 2.4865 1.8082 sec/batch\n",
      "Epoch 29/35  Iteration 170/210 Training loss: 2.4439 1.8276 sec/batch\n",
      "Epoch 29/35  Iteration 171/210 Training loss: 2.4318 1.7736 sec/batch\n",
      "Epoch 29/35  Iteration 172/210 Training loss: 2.4336 1.8027 sec/batch\n",
      "Epoch 29/35  Iteration 173/210 Training loss: 2.4282 1.8045 sec/batch\n",
      "Epoch 29/35  Iteration 174/210 Training loss: 2.4176 1.8291 sec/batch\n",
      "Epoch 30/35  Iteration 175/210 Training loss: 2.4620 1.7887 sec/batch\n",
      "Epoch 30/35  Iteration 176/210 Training loss: 2.4156 1.7659 sec/batch\n",
      "Epoch 30/35  Iteration 177/210 Training loss: 2.4059 1.7651 sec/batch\n",
      "Epoch 30/35  Iteration 178/210 Training loss: 2.4081 1.8053 sec/batch\n",
      "Epoch 30/35  Iteration 179/210 Training loss: 2.4026 1.7639 sec/batch\n",
      "Epoch 30/35  Iteration 180/210 Training loss: 2.3924 1.8788 sec/batch\n",
      "Epoch 31/35  Iteration 181/210 Training loss: 2.4338 1.7633 sec/batch\n",
      "Epoch 31/35  Iteration 182/210 Training loss: 2.3898 1.7791 sec/batch\n",
      "Epoch 31/35  Iteration 183/210 Training loss: 2.3803 1.7712 sec/batch\n",
      "Epoch 31/35  Iteration 184/210 Training loss: 2.3835 1.8122 sec/batch\n",
      "Epoch 31/35  Iteration 185/210 Training loss: 2.3795 1.7700 sec/batch\n",
      "Epoch 31/35  Iteration 186/210 Training loss: 2.3702 1.9103 sec/batch\n",
      "Epoch 32/35  Iteration 187/210 Training loss: 2.4152 1.7938 sec/batch\n",
      "Epoch 32/35  Iteration 188/210 Training loss: 2.3717 1.7651 sec/batch\n",
      "Epoch 32/35  Iteration 189/210 Training loss: 2.3608 1.7844 sec/batch\n",
      "Epoch 32/35  Iteration 190/210 Training loss: 2.3632 1.7767 sec/batch\n",
      "Epoch 32/35  Iteration 191/210 Training loss: 2.3588 1.7922 sec/batch\n",
      "Epoch 32/35  Iteration 192/210 Training loss: 2.3489 1.8470 sec/batch\n",
      "Epoch 33/35  Iteration 193/210 Training loss: 2.3897 1.7898 sec/batch\n",
      "Epoch 33/35  Iteration 194/210 Training loss: 2.3485 1.7817 sec/batch\n",
      "Epoch 33/35  Iteration 195/210 Training loss: 2.3395 1.8044 sec/batch\n",
      "Epoch 33/35  Iteration 196/210 Training loss: 2.3419 1.8977 sec/batch\n",
      "Epoch 33/35  Iteration 197/210 Training loss: 2.3375 1.8012 sec/batch\n",
      "Epoch 33/35  Iteration 198/210 Training loss: 2.3289 1.7894 sec/batch\n",
      "Epoch 34/35  Iteration 199/210 Training loss: 2.3702 1.7888 sec/batch\n",
      "Epoch 34/35  Iteration 200/210 Training loss: 2.3295 1.7608 sec/batch\n",
      "Validation loss: 2.3810284 Saving checkpoint!\n",
      "Epoch 34/35  Iteration 201/210 Training loss: 2.3259 1.9519 sec/batch\n",
      "Epoch 34/35  Iteration 202/210 Training loss: 2.3278 1.8534 sec/batch\n",
      "Epoch 34/35  Iteration 203/210 Training loss: 2.3260 1.7737 sec/batch\n",
      "Epoch 34/35  Iteration 204/210 Training loss: 2.3241 1.7735 sec/batch\n",
      "Epoch 35/35  Iteration 205/210 Training loss: 2.4208 1.7894 sec/batch\n",
      "Epoch 35/35  Iteration 206/210 Training loss: 2.3551 1.7810 sec/batch\n",
      "Epoch 35/35  Iteration 207/210 Training loss: 2.3460 1.8161 sec/batch\n",
      "Epoch 35/35  Iteration 208/210 Training loss: 2.3470 1.8743 sec/batch\n",
      "Epoch 35/35  Iteration 209/210 Training loss: 2.3380 1.7925 sec/batch\n",
      "Epoch 35/35  Iteration 210/210 Training loss: 2.3256 1.7771 sec/batch\n",
      "Validation loss: 2.369336 Saving checkpoint!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Use the line below to load a checkpoint and resume training\n",
    "    #saver.restore(sess, 'checkpoints/______.ckpt')\n",
    "    \n",
    "    n_batches = int(train_x.shape[1]/num_step)\n",
    "    iterations = n_batches * epochs\n",
    "    for e in range(epochs):\n",
    "        \n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for b, (x, y) in enumerate(get_batch([train_x, train_y], num_step), 1):\n",
    "            iteration = e*n_batches + b\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.cost, model.final_state, model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            loss += batch_loss\n",
    "            end = time.time()\n",
    "            print('Epoch {}/{} '.format(e+1, epochs),\n",
    "                  'Iteration {}/{}'.format(iteration, iterations),\n",
    "                  'Training loss: {:.4f}'.format(loss/b),\n",
    "                  '{:.4f} sec/batch'.format((end-start)))\n",
    "        \n",
    "            \n",
    "            if (iteration%save_every_n == 0) or (iteration == iterations):\n",
    "                # Check performance, notice dropout has been set to 1\n",
    "                val_loss = []\n",
    "                new_state = sess.run(model.initial_state)\n",
    "                for x, y in get_batch([test_x, test_y], num_step):\n",
    "                    feed = {model.inputs: x,\n",
    "                            model.targets: y,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    batch_loss, new_state = sess.run([model.cost, model.final_state], feed_dict=feed)\n",
    "                    val_loss.append(batch_loss)\n",
    "\n",
    "                print('Validation loss:', np.mean(val_loss),\n",
    "                      'Saving checkpoint!')\n",
    "                saver.save(sess, \"checkpoints/i{}_l{}_v{:.3f}.ckpt\".format(iteration, lstm_size, np.mean(val_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_top_n(preds, vocab_size, top_n=5):\n",
    "    p = np.squeeze(preds)\n",
    "    p[np.argsort(p)[:-top_n]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    c = np.random.choice(vocab_size, 1, p=p)[0]\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n",
    "    samples = [c for c in prime]\n",
    "    model = build_rnn(vocab_size, lstm_size=lstm_size, sampling=True)\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess, checkpoint)\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        for c in prime:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0,0] = vocab_to_int[c]\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "        c = pick_top_n(preds, len(vocab))\n",
    "        samples.append(int_to_vocab[c])\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            x[0,0] = c\n",
    "            feed = {model.inputs: x,\n",
    "                    model.keep_prob: 1.,\n",
    "                    model.initial_state: new_state}\n",
    "            preds, new_state = sess.run([model.preds, model.final_state], \n",
    "                                         feed_dict=feed)\n",
    "\n",
    "            c = pick_top_n(preds, len(vocab))\n",
    "            samples.append(int_to_vocab[c])\n",
    "        \n",
    "    return ''.join(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_checkpoint_path: \"checkpoints/i210_l512_v2.369.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i200_l512_v2.381.ckpt\"\n",
       "all_model_checkpoint_paths: \"checkpoints/i210_l512_v2.369.ckpt\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.get_checkpoint_state('checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from checkpoints/i210_l512_v2.369.ckpt\n",
      "ok stisengentit ant estaistoonen ate tom lo tes ang inthe sot in the sotined ondith ion thandiling to the serat ing out tomprot thin te thes inserall tor ined thon sote tiat of welere the sicanenthat esterit anto th t mest in atee tes chille toretins ther int ous out it on an oo ply the tand tom leth inge tite ins thes tereste to chetarit oo ser issestis the the dat int tam te andinte ale sat on and to sere sice the d an at outing sichit are satang ar ondaly youstis ist ant in th the thes at ond ta t at te se cally th medate ta the the calil  of ing setating an tas ine there atine ane deandithendet ar on the  or ing ar inet in sing tes the  insuran in to the thoulis oususethat or in iondit and thin th ce s ine caster ce s at iou areall to s an tou ly ureangon tores alitingite areing teat aset eas ou ate th ing sat tore cat ingeas ont on wat ing itit set astiset oo lis an te the datisict iner asu te th ie anengo cone tat ous io the too mouse aodelyor ing on titis sompllete antor casten titi is the cer thes mall ort ing te an tamen th  eally th the tariil tha tions istain io tor io the tais to cor tat to  soule sas ce arathe the sanin the conens tounte and an the site sit ane tathe arn on ioun in the cones tiou le sisit an angereal ofeal ar ofe s and tat on ing tharn tise tor cout tas ing tion ine poren the tiseling ine ang the tout as ing istine ast ine cate then ong ta thor ist met to s thit this in tare th to tor thes ton asuris an ine th in thes an is tit ale setathe doat ioul an thand th tisin is ast of arethe tho d to caling tean ing te all ar ing the ant artes ar ifter ise is thit atise areine atation soredin  istong an the ane the thi g ale seat in tore co tat in thas ant ting so thet or ing sedere ar itis co pee so tamplodane to in the te andini ta co the arining ing steate tinting ing te to sit se tho  at ar se that an ing soun the se to suse the siticing of the thas too the tine andise te and is ane ses all oo  oou the thin ale as allonereanithe angereangitios\n"
     ]
    }
   ],
   "source": [
    "# Change the name of latest checkpoint accordingly\n",
    "checkpoint = \"checkpoints/i210_l512_v2.369.ckpt\"\n",
    "samp = sample(checkpoint,2000, lstm_size, len(vocab), prime=\"ok\")\n",
    "print(samp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion : Seems like prediction is not well need more training and generic one to train our model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
