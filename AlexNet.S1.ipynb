{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-worcester",
   "metadata": {},
   "source": [
    "### AlexNet Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "leading-lambda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tflearn\n",
      "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
      "\u001b[K     |████████████████████████████████| 107 kB 1.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/nick/opt/anaconda3/envs/Tensorflow/lib/python3.8/site-packages (from tflearn) (1.19.5)\n",
      "Requirement already satisfied: six in /Users/nick/opt/anaconda3/envs/Tensorflow/lib/python3.8/site-packages (from tflearn) (1.15.0)\n",
      "Requirement already satisfied: Pillow in /Users/nick/opt/anaconda3/envs/Tensorflow/lib/python3.8/site-packages (from tflearn) (8.1.0)\n",
      "Building wheels for collected packages: tflearn\n",
      "  Building wheel for tflearn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127300 sha256=526d60825576b42e950648bc0d2e8fcd0bd540df3bbff6762af58278577452ba\n",
      "  Stored in directory: /Users/nick/Library/Caches/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
      "Successfully built tflearn\n",
      "Installing collected packages: tflearn\n",
      "Successfully installed tflearn-0.5.0\n"
     ]
    }
   ],
   "source": [
    "# installing tflearn dataset for oxflowers17 dataset\n",
    "\n",
    "!pip install tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecological-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing important packages\n",
    "\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D,Dense,Flatten,MaxPooling2D,Activation,Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import tflearn.datasets.oxflower17 as oxflower17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "touched-cambodia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating x and y\n",
    "\n",
    "x,y = oxflower17.load_data(one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "elder-pontiac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_33 (Conv2D)           (None, 54, 54, 96)        34944     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 54, 54, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 52, 52, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 52, 52, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_34 (Conv2D)           (None, 24, 24, 256)       614656    \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 24, 24, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 11, 11, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 11, 11, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_35 (Conv2D)           (None, 9, 9, 384)         885120    \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 9, 9, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_46 (Batc (None, 9, 9, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_36 (Conv2D)           (None, 7, 7, 384)         1327488   \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 7, 7, 384)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_47 (Batc (None, 7, 7, 384)         1536      \n",
      "_________________________________________________________________\n",
      "conv2d_37 (Conv2D)           (None, 6, 6, 256)         393472    \n",
      "_________________________________________________________________\n",
      "activation_50 (Activation)   (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_48 (Batc (None, 5, 5, 256)         1024      \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4096)              26218496  \n",
      "_________________________________________________________________\n",
      "activation_51 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_49 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_52 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_50 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_51 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 17)                17017     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 17)                0         \n",
      "=================================================================\n",
      "Total params: 50,411,777\n",
      "Trainable params: 50,390,641\n",
      "Non-trainable params: 21,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# creating AlexNet Architecture\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# creating convolutional layer\n",
    "\n",
    "model.add(Conv2D(filters = 96,input_shape = (224,224,3),kernel_size = (11,11), strides = (4,4),padding = 'valid'))\n",
    "\n",
    "# adding activation for normalization\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Adding overlapping pooling layer\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (3,3),strides = (1,1), padding = 'valid'))\n",
    "\n",
    "# adding Batch Normalization to normalize all arrays one after another to the depth\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# adding 2nd convolution layer\n",
    "\n",
    "model.add(Conv2D(filters = 256,kernel_size = (5,5), strides = (2,2),padding = 'valid'))\n",
    "\n",
    "# adding activation function\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Adding pooling layer\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (3,3),strides = (2,2), padding = 'valid'))\n",
    "\n",
    "# adding Normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding 3rd convolution layer\n",
    "\n",
    "model.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'valid'))\n",
    "\n",
    "# adding activation function\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding 4th convolution layer\n",
    "\n",
    "model.add(Conv2D(filters = 384, kernel_size = (3,3), strides = (1,1), padding = 'valid'))\n",
    "\n",
    "# adding activation function\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Adding 5th convolution layer\n",
    "\n",
    "model.add(Conv2D(filters = 256, kernel_size = (2,2), strides = (1,1), padding = 'valid'))\n",
    "\n",
    "# adding activation function\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding pooling layer\n",
    "\n",
    "model.add(MaxPooling2D(pool_size = (2,2), strides = (1,1), padding = 'valid'))\n",
    "\n",
    "# adding normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# creating Flatten Layer\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# creating 1st Dense Layer\n",
    "\n",
    "model.add(Dense(4096,input_shape = (224*224*3,)))\n",
    "\n",
    "# adding activation \n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding dropout for the first time ever to avoide overfitting\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# adding batch normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# creating 2nd Dense Layer\n",
    "\n",
    "model.add(Dense(4096))\n",
    "\n",
    "# adding activation \n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding dropout for the first time ever to avoide overfitting\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# adding batch normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# creating 3rd Dense Layer\n",
    "\n",
    "model.add(Dense(1000))\n",
    "\n",
    "# adding activation \n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# adding dropout for the first time ever to avoide overfitting\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# adding batch normalization\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# output layer\n",
    "\n",
    "model.add(Dense(17))\n",
    "\n",
    "# output activation layer\n",
    "\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cultural-crown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "found-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1088 samples, validate on 272 samples\n",
      "Epoch 1/5\n",
      "1088/1088 [==============================] - 72s 67ms/sample - loss: 1.5101 - acc: 0.5156 - val_loss: 50.9003 - val_acc: 0.0735\n",
      "Epoch 2/5\n",
      "1088/1088 [==============================] - 66s 61ms/sample - loss: 1.3136 - acc: 0.5800 - val_loss: 11.9684 - val_acc: 0.1765\n",
      "Epoch 3/5\n",
      "1088/1088 [==============================] - 74s 68ms/sample - loss: 1.1136 - acc: 0.6415 - val_loss: 13.5072 - val_acc: 0.1287\n",
      "Epoch 4/5\n",
      "1088/1088 [==============================] - 75s 68ms/sample - loss: 1.0579 - acc: 0.6434 - val_loss: 4.2122 - val_acc: 0.3015\n",
      "Epoch 5/5\n",
      "1088/1088 [==============================] - 68s 62ms/sample - loss: 1.0211 - acc: 0.6682 - val_loss: 4.5309 - val_acc: 0.3162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe64d0db550>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the model\n",
    "\n",
    "model.fit(x,y,batch_size = 64,epochs = 5,validation_split = 0.2,shuffle = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
